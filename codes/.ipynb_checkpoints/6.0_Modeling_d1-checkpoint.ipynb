{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max.columns\", None)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from tapy import Indicators\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Dropout, GRU\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D1 Timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in 2019 full year 1 hour timeframe price data\n",
    "df_d1_feature = pd.read_csv('../data/df_d1_feature.csv', index_col=0)\n",
    "df_d1_feature.index = pd.to_datetime(df_d1_feature.index)\n",
    "df_d1_feature = df_d1_feature.dropna()\n",
    "df_d1_feature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d1_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d1_feature.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_feature = ['open', 'high', 'low', 'close', 'results',\n",
    "               'shift_01', 'shift_02', 'shift_03', 'shift_05', \n",
    "               'shift_08', 'shift_13', 'shift_21', 'shift_34',\n",
    "               'shift_55', 'shift_89'\n",
    "               'sma_02', 'sma_03', 'sma_05', 'sma_08', \n",
    "               'sma_13', 'sma_21', 'sma_34', 'sma_55', 'sma_89',\n",
    "               'diff','gain','loss']\n",
    "features = [col for col in df_d1_feature.columns if col not in non_feature]\n",
    "\n",
    "X = df_d1_feature[features]\n",
    "y = df_d1_feature['results']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "X_train_ss = ss.fit_transform(X_train)\n",
    "X_test_ss = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### un-randomized train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Gridsearch parameters\n",
    "logr_params = {'C':np.logspace(-1,10),\n",
    "               'class_weight': [None, 'balanced'],\n",
    "               'penalty':['l1', 'l2'],}\n",
    "\n",
    "# Score base on roc_auc\n",
    "logr_gridsearch = RandomizedSearchCV(LogisticRegression(solver='liblinear'), \n",
    "                              logr_params,\n",
    "                              cv=5,\n",
    "                              verbose=1,                              \n",
    "                              n_jobs=-1)\n",
    "\n",
    "logr_gridsearch = logr_gridsearch.fit(X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"cv best score mean: {round(logr_gridsearch.best_score_, 4)}\")\n",
    "print(f\"cv best score std: {round(logr_gridsearch.cv_results_['std_test_score'][logr_gridsearch.best_index_], 4)}\")\n",
    "print(f\"cv best param: {logr_gridsearch.best_params_}\")\n",
    "print(\"\")\n",
    "print(f\"train set score: {round(logr_gridsearch.score(X_train_ss, y_train), 4)}\")\n",
    "print(f\"validation set score: {round(logr_gridsearch.score(X_test_ss, y_test), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr_pred = logr_gridsearch.predict(X_test_ss)\n",
    "cm = confusion_matrix(y_test, logr_pred)\n",
    "cm_df = pd.DataFrame(cm, columns=['pred 0', 'pred 1', 'pred 2'], index=['actual 0', 'actual 1', 'actual 2'])\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_df.loc['actual 1'][1]/(cm_df['pred 1'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_df.loc['actual 2'][2]/(cm_df['pred 2'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cm_df.loc['actual 1'][1]+cm_df.loc['actual 2'][2])/((cm_df['pred 1'].sum())+(cm_df['pred 2'].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### un-randomized train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rf_params = {\n",
    "    'n_estimators': list(range(400, 601)),\n",
    "    'min_samples_split': list(range(2, 9)),\n",
    "    'max_features' : list(range(1, X_train_ss.shape[1])),\n",
    "    'max_depth': list(range(2, 10)),\n",
    "    'class_weight': [None, 'balanced', 'balanced_subsample']\n",
    "}\n",
    "\n",
    "rf_gridsearch = RandomizedSearchCV(RandomForestClassifier(), \n",
    "                              rf_params,\n",
    "                              cv=5, \n",
    "                              verbose=1,\n",
    "                              n_iter=10,\n",
    "                              n_jobs=-1)\n",
    "\n",
    "rf_gridsearch = rf_gridsearch.fit(X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"cv best score mean: {round(rf_gridsearch.best_score_, 4)}\")\n",
    "print(f\"cv best score std: {round(rf_gridsearch.cv_results_['std_test_score'][rf_gridsearch.best_index_], 4)}\")\n",
    "print(f\"cv best param: {rf_gridsearch.best_params_}\")\n",
    "print(\"\")\n",
    "print(f\"train set score: {round(rf_gridsearch.score(X_train_ss, y_train), 4)}\")\n",
    "print(f\"validation set score: {round(rf_gridsearch.score(X_test_ss, y_test), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred = rf_gridsearch.predict(X_test_ss)\n",
    "cm = confusion_matrix(y_test, rf_pred)\n",
    "cm_df = pd.DataFrame(cm, columns=['pred 0', 'pred 1', 'pred 2'], index=['actual 0', 'actual 1', 'actual 2'])\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_df.loc['actual 1'][1]/(cm_df['pred 1'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_df.loc['actual 2'][2]/(cm_df['pred 2'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cm_df.loc['actual 1'][1]+cm_df.loc['actual 2'][2])/((cm_df['pred 1'].sum())+(cm_df['pred 2'].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extremely Randomized Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### un-randomized train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "et_params = {\n",
    "    'n_estimators': list(range(400, 601)),\n",
    "    'min_samples_split': list(range(2, 9)),    \n",
    "    'max_features' : list(range(1, X_train_ss.shape[1])),\n",
    "    'max_depth': list(range(2, 10)),\n",
    "    'class_weight': [None, 'balanced', 'balanced_subsample']\n",
    "}\n",
    "\n",
    "et_gridsearch = RandomizedSearchCV(ExtraTreesClassifier(), \n",
    "                              et_params,\n",
    "                              cv=5, \n",
    "                              verbose=1, \n",
    "                              n_jobs=-1)\n",
    "\n",
    "et_gridsearch = et_gridsearch.fit(X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"cv best score mean: {round(et_gridsearch.best_score_, 4)}\")\n",
    "print(f\"cv best score std: {round(et_gridsearch.cv_results_['std_test_score'][et_gridsearch.best_index_], 4)}\")\n",
    "print(f\"cv best param: {et_gridsearch.best_params_}\")\n",
    "print(\"\")\n",
    "print(f\"train set score: {round(et_gridsearch.score(X_train_ss, y_train), 4)}\")\n",
    "print(f\"validation set score: {round(et_gridsearch.score(X_test_ss, y_test), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "et_pred = et_gridsearch.predict(X_test_ss)\n",
    "cm = confusion_matrix(y_test, et_pred)\n",
    "cm_df = pd.DataFrame(cm, columns=['pred 0', 'pred 1', 'pred 2'], index=['actual 0', 'actual 1', 'actual 2'])\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_df.loc['actual 1'][1]/(cm_df['pred 1'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_df.loc['actual 2'][2]/(cm_df['pred 2'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cm_df.loc['actual 1'][1]+cm_df.loc['actual 2'][2])/((cm_df['pred 1'].sum())+(cm_df['pred 2'].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green>XGBoost</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### un-randomized train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xgc_param = [{'subsample' : [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],\n",
    "              'reg_lambda' : np.logspace(-5,5),\n",
    "              'reg_alpha' : np.logspace(-5,5),\n",
    "              'max_depth' : list(range(2, 10)),\n",
    "              'learning_rate' : [0.0001],\n",
    "              'gamma' : np.logspace(-5,5),\n",
    "              'colsample_bytree' : np.logspace(-5,5), }]\n",
    "\n",
    "xgc_gridsearch = RandomizedSearchCV(XGBClassifier(), \n",
    "                              xgc_param,\n",
    "                              cv=5, \n",
    "                              verbose=1, \n",
    "                              n_jobs=-1)\n",
    "\n",
    "xgc_gridsearch = xgc_gridsearch.fit(X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"cv best score mean: {round(xgc_gridsearch.best_score_, 4)}\")\n",
    "print(f\"cv best score std: {round(xgc_gridsearch.cv_results_['std_test_score'][xgc_gridsearch.best_index_], 4)}\")\n",
    "print(f\"cv best param: {xgc_gridsearch.best_params_}\")\n",
    "print(\"\")\n",
    "print(f\"train set score: {round(xgc_gridsearch.score(X_train_ss, y_train), 4)}\")\n",
    "print(f\"validation set score: {round(xgc_gridsearch.score(X_test_ss, y_test), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgc_pred = xgc_gridsearch.predict(X_test_ss)\n",
    "cm = confusion_matrix(y_test, xgc_pred)\n",
    "cm_df = pd.DataFrame(cm, columns=['pred 0', 'pred 1', 'pred 2'], index=['actual 0', 'actual 1', 'actual 2'])\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_df.loc['actual 1'][1]/(cm_df['pred 1'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_df.loc['actual 2'][2]/(cm_df['pred 2'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cm_df.loc['actual 1'][1]+cm_df.loc['actual 2'][2])/((cm_df['pred 1'].sum())+(cm_df['pred 2'].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### un-randomized train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cat = to_categorical(y_train)\n",
    "y_test_cat = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(X_train_ss.shape[1], \n",
    "                input_shape= (X_train_ss.shape[1],),\n",
    "                activation='relu',\n",
    "                kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=Adam(lr = .0005), \n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit(X_train_ss, y_train_cat, \n",
    "                    validation_data=(X_test_ss, y_test_cat), \n",
    "                    epochs=50, batch_size=256,verbose=1,\n",
    "                    callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='Train loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['categorical_accuracy'], label='Train accuracy')\n",
    "plt.plot(history.history['val_categorical_accuracy'], label='val_categorical_accuracy accuracy')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnn_pred = model.predict_classes(X_test_ss)\n",
    "cm = confusion_matrix(y_test, fnn_pred)\n",
    "cm_df = pd.DataFrame(cm, columns=['pred 0', 'pred 1', 'pred 2'], index=['actual 0', 'actual 1', 'actual 2'])\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_df.loc['actual 1'][1]/(cm_df['pred 1'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_df.loc['actual 2'][2]/(cm_df['pred 2'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cm_df.loc['actual 1'][1]+cm_df.loc['actual 2'][2])/((cm_df['pred 1'].sum())+(cm_df['pred 2'].sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
